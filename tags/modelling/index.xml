<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Modelling on PBenavides</title>
    <link>https://pbenavides.github.io/tags/modelling/</link>
    <description>Recent content in Modelling on PBenavides</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Aug 2021 11:00:59 -0400</lastBuildDate><atom:link href="https://pbenavides.github.io/tags/modelling/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Credit Fraud API</title>
      <link>https://pbenavides.github.io/post/credit-fraud-api/</link>
      <pubDate>Tue, 10 Aug 2021 11:00:59 -0400</pubDate>
      
      <guid>https://pbenavides.github.io/post/credit-fraud-api/</guid>
      <description>Credit Fraud Analysis. This was made based on this Kaggle dataset of fraud-transactions. The dataset is well known but this approach is considering the model as a service to predict fast and accurate fraud transactions. I personally think a deploy approach is nearest to the real-world application rather than just optimizing the metrics.
The entire process could be explained as the following:
From Notebooks:
1.- Split dataset Notebook
  Since Time was measured with respect to the first transaction, it has to be transformed with a cyclical method to have the same measure in a production environment.</description>
    </item>
    
  </channel>
</rss>
